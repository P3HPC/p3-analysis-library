{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Understanding Data Projection\n\nProjecting data onto P3 definitions.\n\nThe P3 Analysis Library expects data to be prepared in a specific\n`format <performance_data>`. This format was inspired by the\n`terminology <terminology>` first introduced in \"`Implications of a Metric\nfor Performance Portability`_\":\n\n   https://doi.org/10.1016/j.future.2017.08.007\n\n**Problem**\n  A task with a pass/fail metric for which quantitative performance may be\n  measured. Multiplying an $N \\times K$ matrix by a $K \\times M$\n  matrix to the accuracy guaranteed by IEEE 754 double precision, computing\n  $\\pi$ to a certain number of decimal places, and sorting an array of\n  $N$ elements are all examples of problems.\n\n**Application**\n  Software capable of solving a *problem* with measurable correctness and\n  performance. Math libraries, Python scripts, C functions, and entire software\n  packages are all examples of applications; the Intel |reg| oneAPI Math Kernel\n  Library is an example of an application for solving linear algebra problems.\n\n.. |reg| unicode:: U+00AE\n   :ltrim:\n\n**Platform**\n  A collection of software **and** hardware on which an *application* may run a\n  *problem*. A specific processor coupled with an operating system, compiler,\n  runtime, drivers, library dependencies, etc is an example of a precise\n  platform definition.\n\nThese definitions are flexible, allowing the same performance data to be used\nfor multiple case studies with different interpretations of these terms.\n\nRather than store raw performance data in columns corresponding to these\ndefinitions, the P3 Analysis Library provides functionality to *project* raw\nperformance data onto specific meanings of \"problem\", \"application\" and\n\"platform\".\n\n## Using Projection to Rename Columns\n\nThe simplest example of projection is a straightforward renaming of columns.\n\nLet's assume that we've collected some performance data from a few different\nimplementations of a function, running a number of problem sizes on multiple\nmachines.\n\n.. important::\n    Although we are looking at \"function\" performance here, the concepts\n    generalize to entire software packages.\n\nOur raw performance data might look like this:\n\n.. list-table::\n    :widths: 20 20 20 20\n    :header-rows: 1\n\n    * - size\n      - implementation\n      - machine\n      - fom\n\n    * - 128x128x128\n      - Library 1\n      - Cluster 1\n      - 0.5\n\n    * - 256x256x256\n      - Library 1\n      - Cluster 1\n      - 2.0\n\n    * - 128x128x128\n      - Library 2\n      - Cluster 1\n      - 0.7\n\n    * - 256x256x256\n      - Library 2\n      - Cluster 1\n      - 2.1\n\n    * - 128x128x128\n      - Library 1\n      - Cluster 2\n      - 0.25\n\n    * - 256x256x256\n      - Library 1\n      - Cluster 2\n      - 1.0\n\n    * - 128x128x128\n      - Library 2\n      - Cluster 2\n      - 0.125\n\n    * - 256x256x256\n      - Library 2\n      - Cluster 2\n      - 0.5\n\nThe most obvious projection of this data onto P3 definitions is as follows:\n\n- Each input size maps to a different problem, because each input\n  represents a different task to be solved, with its own expected answer to\n  validate against.\n\n- Each implementation maps to a different application, because each\n  library's implementation of the function produces a solution for a given\n  input with measurable performance and correctness.\n\n- Each machine maps to a different platform, because each cluster name\n  describes the combination of hardware **and** software used to run the\n  experiments.\n\n.. important::\n    In reality, a single value is unlikely to provide enough information to\n    fully and unambiguously describe a function's behavior, its implementation,\n    or the state of a machine when its performance was recorded. But we'll come\n    back to that later.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After loading our data into a :py:class:`pandas.DataFrame`\n(``df``), we can use the\n:py:func:`p3analysis.data.projection` function to perform this\nprojection, renaming the columns as described above.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "proj = p3analysis.data.projection(\n    df,\n    problem=[\"size\"],\n    application=[\"implementation\"],\n    platform=[\"machine\"],\n)\nprint(proj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Following projection, our performance data is now ready to be passed to\nfunctions in the :py:mod:`p3analysis.metrics` module.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Projection to Combine Columns\n\nAs we alluded to earlier, it's unlikely that a single column of the raw data\nfully captures the definition of a \"problem\", \"application\" or \"platform\".\n\nLet's make our raw data slightly more complicated, by introducing the notion\nthat the function of interest is available in both single precision (FP32)\nand double precision (FP64).\n\n.. list-table::\n    :widths: 20 20 20 20 20\n    :header-rows: 1\n\n    * - size\n      - precision\n      - implementation\n      - machine\n      - fom\n\n    * - 128x128x128\n      - FP32\n      - Library 1\n      - Cluster 1\n      - 0.5\n\n    * - 256x256x256\n      - FP32\n      - Library 1\n      - Cluster 1\n      - 2.0\n\n    * - 128x128x128\n      - FP32\n      - Library 2\n      - Cluster 1\n      - 0.7\n\n    * - 256x256x256\n      - FP32\n      - Library 2\n      - Cluster 1\n      - 2.1\n\n    * - 128x128x128\n      - FP32\n      - Library 1\n      - Cluster 2\n      - 0.25\n\n    * - 256x256x256\n      - FP32\n      - Library 1\n      - Cluster 2\n      - 1.0\n\n    * - 128x128x128\n      - FP32\n      - Library 2\n      - Cluster 2\n      - 0.125\n\n    * - 256x256x256\n      - FP32\n      - Library 2\n      - Cluster 2\n      - 0.5\n\n    * - 128x128x128\n      - FP64\n      - Library 1\n      - Cluster 1\n      - 1.0\n\n    * - 256x256x256\n      - FP64\n      - Library 1\n      - Cluster 1\n      - 4.0\n\n    * - 128x128x128\n      - FP64\n      - Library 2\n      - Cluster 1\n      - 1.4\n\n    * - 256x256x256\n      - FP64\n      - Library 2\n      - Cluster 1\n      - 4.2\n\n    * - 128x128x128\n      - FP64\n      - Library 1\n      - Cluster 2\n      - 0.5\n\n    * - 256x256x256\n      - FP64\n      - Library 1\n      - Cluster 2\n      - 2.0\n\n    * - 128x128x128\n      - FP64\n      - Library 2\n      - Cluster 2\n      - 0.25\n\n    * - 256x256x256\n      - FP64\n      - Library 2\n      - Cluster 2\n      - 1.0\n\nHow does this impact our projection? The implementation and machine columns\nare still enough to describe the application and platform (respectively),\nbut what about the problem? The answer is, of course: \"It depends\".\n\nLuckily, this dataset is simple enough that we can enumerate our options:\n\n1. Each unique (size, precision) tuple maps to a different problem,\n   representing that the problem definition requires the task to be solved to\n   a specific precision (and that the precision has a material impact on the\n   verification of results).\n\n2. Each size maps to a different problem as before, representing that the\n   problem definition does **not** require the task to be solved to any\n   specific precision, and that implementations are free to select whichever\n   precision delivers the best performance.\n\nNeither of these options is more correct than the other. Rather, they\nrepresent different studies.\n\nBoth projections can be performed with the :py:func:`p3analysis.data.projection`\nfunction, by passing different arguments.\n\nFor the first projection, we now need to specify the names of two columns\n(\"size\" and \"precision\") to define the problem:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "proj1 = p3analysis.data.projection(\n    df,\n    problem=[\"size\", \"precision\"],\n    application=[\"implementation\"],\n    platform=[\"machine\"],\n)\nprint(proj1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The original \"size\" and \"precision\" columns have been removed, and their\nvalues have been concatenated to form the new \"problem\" column.\n\nFor the second projection, we just need to specify \"size\", exactly as we did\nbefore:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "proj2 = p3analysis.data.projection(\n    df,\n    problem=[\"size\"],\n    application=[\"implementation\"],\n    platform=[\"machine\"],\n)\nprint(proj2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This time, the original \"size\" column has been removed, but the \"precision\"\ncolumn remains.\n\nClearly, the values provided to the projection function change the structure\nof the resulting :py:class:`pandas.DataFrame`. But why does that matter?\nWell, let's take a look at what happens if we compute the maximum \"fom\" for\neach (problem, application, platform) tuple in our projected datasets:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max1 = proj1.groupby([\"problem\", \"application\", \"platform\"])[\"fom\"].max()\nprint(max1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max2 = proj2.groupby([\"problem\", \"application\", \"platform\"])[\"fom\"].max()\nprint(max2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar :py:func:`pandas.DataFrame.groupby` calls form the backbone of many\nfunctions provided by the P3 Analysis Library, since metrics like\n\"application efficiency\" and \"performance portability\" ultimately depend on\nan understanding of which variable combinations deliver the best performance.\n\n.. important::\n    The selected projection can have significant impact on the results of\n    subsequent analysis, and it is critical to ensure that the projection\n    is correct before digging too deep into (or presenting!) any results.\n\n## Next Steps\n\nAfter raw performance data has been projected onto definitions\nof \"problem\", \"application\", and \"platform\", it can be passed to\nany of the P3 Analysis Library functions that expect a\n:py:class:`pandas.DataFrame`.\n\nThe examples below show how to use projected data to compute\nand visualize derived metrics like \"application efficiency\",\n\"performance portability\", and \"code divergence\".\n\n.. minigallery::\n    :add-heading: Examples\n\n    ../../examples/metrics/application_efficiency.py\n    ../../examples/cascade/plot_simple_cascade.py\n    ../../examples/navchart/plot_simple_navchart.py\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}