{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Handling Software with Multiple Components\n\nViewing applications as composites.\n\nWhen working with very large and complex pieces of software, reporting\nperformance using a single number (e.g., total time-to-solution) obscures\ndetails about the performance of different software components. Using such\ntotals during P3 analysis therefore prevents us from understanding how\ndifferent software components behave on different platforms.\n\nIdentifying which software components have poor P3 characteristics is necessary\nto understand what action(s) we can take to improve the P3 characteristics of\na software package as a whole. Although accounting for multiple components can\nmake data collection and analysis slightly more complicated, the additional\ninsight it provides is very valuable.\n\n.. tip::\n    This approach can be readily applied to parallel software written to\n    heterogeneous programming frameworks (e.g., CUDA, OpenCL, SYCL, Kokkos),\n    where distinct \"kernel\"s can be identified and profiled easily. For a\n    real-life example of this approach in practice, see \"[A\n    Performance-Portable SYCL Implementation of CRK-HACC for Exascale](https://dl.acm.org/doi/10.1145/3624062.3624187).\n\n## Data Preparation\n\nTo keep things simple, let's imagine that our software package consists of just\ntwo components, and that each component has two different implementations that\ncan both be run on two different machines:\n\n .. list-table::\n     :widths: 20 20 20 20\n     :header-rows: 1\n\n     * - component\n       - implementation\n       - machine\n       - fom\n\n     * - Component 1\n       - Implementation 1\n       - Cluster 1\n       - 2.0\n\n     * - Component 2\n       - Implementation 1\n       - Cluster 1\n       - 5.0\n\n     * - Component 1\n       - Implementation 2\n       - Cluster 1\n       - 3.0\n\n     * - Component 2\n       - Implementation 2\n       - Cluster 1\n       - 4.0\n\n     * - Component 1\n       - Implementation 1\n       - Cluster 2\n       - 1.0\n\n     * - Component 2\n       - Implementation 1\n       - Cluster 2\n       - 2.5\n\n     * - Component 1\n       - Implementation 2\n       - Cluster 2\n       - 0.5\n\n     * - Component 1\n       - Implementation 2\n       - Cluster 2\n       - 3.0\n\nOur first step is to project this data onto P3 definitions,\ntreating the functionality provided by each component as a\nseparate problem to be solved:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "proj = p3analysis.data.projection(\n    df,\n    problem=[\"component\"],\n    application=[\"implementation\"],\n    platform=[\"machine\"],\n)\nprint(proj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>See \"`Understanding Data Projection <understanding_projection>`\" for\n    more information about projection.</p></div>\n\n## Application Efficiency per Component\n\nHaving projected the performance data onto P3 definitions, we can now compute\nthe application efficiency for each component:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "effs = p3analysis.metrics.application_efficiency(proj)\nprint(effs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>See \"`Working with Application Efficiency\n    <working_with_app_efficiency>`\" for more information about application\n    efficiency.</p></div>\n\nPlotting a graph for each platform separately is a good way to visualize and\ncompare the application efficiency of each component:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cluster1 = effs[effs[\"platform\"] == \"Cluster 1\"]\npivot = cluster1.pivot(index=\"application\", columns=[\"problem\"])[\"app eff\"]\npivot.plot(\n    kind=\"bar\",\n    xlabel=\"Component\",\n    ylabel=\"Application Efficiency\",\n    title=\"Cluster 1\",\n)\nplt.savefig(\"cluster1_application_efficiency_bars.png\")\n\ncluster2 = effs[effs[\"platform\"] == \"Cluster 2\"]\npivot = cluster2.pivot(index=\"application\", columns=[\"problem\"])[\"app eff\"]\npivot.plot(\n    kind=\"bar\",\n    xlabel=\"Component\",\n    ylabel=\"Application Efficiency\",\n    title=\"Cluster 2\",\n)\nplt.savefig(\"cluster2_application_efficiency_bars.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On Cluster 1, Implementation 1 delivers the best performance for Component 1,\nbut Implementation 2 delivers the best performance for Component 2. On\nCluster 2, that trend is reversed. Clearly, there is no single implementation\nthat delivers the best performance everywhere.\n\n## Overall Application Efficiency\n\nComputing the application efficiency of the software package as a whole\nrequires a few more steps.\n\nFirst, we need to compute the total time taken by each application on each\nplatform:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "package = proj.groupby([\"platform\", \"application\"], as_index=False)[\"fom\"].sum()\npackage[\"problem\"] = \"Package\"\nprint(package)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can use this data to compute application efficiency, as below:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "effs = p3analysis.metrics.application_efficiency(package)\nprint(effs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These latest results suggest that both Implementation 1 and Implementation 2\nare both achieving the best-known performance when running the package as a\nwhole. This isn't *strictly* incorrect, since the values of their combined\nfigure-of-merit *are* the same, but we know from our earlier per-component\nanalysis that it could be possible to achieve better performance results.\n\nSpecifically, our per-component analysis shows us that an application that\ncould pick and choose the best implementation of different components for\ndifferent platforms would achieve better overall performance.\n\n.. important::\n    Combining component implementations in this way is purely hypothetical,\n    and there may be very good reasons (e.g., incompatible data structures)\n    that an application is unable to use certain combinations. Although\n    removing such invalid combinations would result in a tighter upper\n    bound, it is much simpler to leave them in place. Including all\n    combinations may even identify potential opportunities to combine\n    approaches that initially appeared incompatible (e.g., by writing\n    routines to convert between data structures).\n\nWe can fold that observation into our P3 analysis by creating an entry in our\ndataset that represents the results from a hypothetical application:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hypothetical_components = proj.groupby([\"problem\", \"platform\"], as_index=False)[\n    \"fom\"\n].min()\nhypothetical_components[\"application\"] = \"Hypothetical\"\nprint(hypothetical_components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Calculate the combined figure of merit for both components\nhypothetical_package = hypothetical_components.groupby(\n    [\"platform\", \"application\"], as_index=False,\n)[\"fom\"].sum()\nhypothetical_package[\"problem\"] = \"Package\"\n\n# Append the hypothetical package data to our previous results\npackage = pd.concat([package, hypothetical_package], ignore_index=True)\nprint(package)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, our new hypothetical application achieves better performance\nby mixing and matching different implementations. And if we now re-compute\napplication efficiency with this data included:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "effs = p3analysis.metrics.application_efficiency(package)\nprint(effs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "... we see that the application efficiency of Implementation 1 and\nImplementation 2 has been reduced accordingly. Including hypothetical\nupper-bounds of performance in our dataset can therefore be a simple and\neffective way to improve the accuracy of our P3 analysis, even if a\ntrue theoretical upper-bound (i.e., from a performance model) is unknown.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The two implementations still have the *same* efficiency, even after\n    introducing the hypothetical implementation. Per-component analysis is\n    still required to understand how each component contributes to the\n    overall efficiency, and to identify which component(s) should be improved\n    on which platform(s).</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further Analysis\n\nComputing application efficiency is often simply the first step of a\nmore detailed P3 analysis.\n\nThe examples below show how we can use the visualization capabilities\nof the P3 Analysis Library to compare the efficiency of different\napplications running across the same platform set, or to gain insight\ninto how an application's efficiency relates to the code it uses on each\nplatform.\n\n.. minigallery::\n    :add-heading: Examples\n\n    ../../examples/cascade/plot_simple_cascade.py\n    ../../examples/navchart/plot_simple_navchart.py\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}